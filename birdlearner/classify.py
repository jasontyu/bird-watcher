from nltk.tokenize import TweetTokenizer
from math import log
tokenizer = TweetTokenizer(strip_handles=False, preserve_case=False)

from categories import happysad

def tokens_with_entities_replaced(tokens):
    """
    In a tokenized string, replace all twitter entities with meta words.

    Args:
        tokens (:obj: `list` of `str`): Tokenized string, formerly the text attribute of a
            tweet object.

    Returns:
        :obj: `list` of `str`: Tokenized string with all twitter entities replaced by meta
            words.

    Notes:
        Meta words:
            [USER] -> denotes a user mention (eg @user)
            -DEPRECATED- [HASHTAG] -> denotes hashtag (eg #hashtag)
            [URL] -> denotes url (eg https://t.co/hyperlink)
    """
    for i,token in enumerate(tokens):
        if token[0] == "@":
            tokens[i] = u"[USER]"
        elif token[0] == "#":
            # tokens[i] = "[HASHTAG]"
            pass
        elif u"http" in token:
            tokens[i] = u"[URL]"
        #elif token in happysad:
        #    tokens[i] = u"[EMOJI]"
        else:
            continue
    return tokens


def tokenize(text):
    """
    Split a string of text into tokens and replace all twitter entities with meta-words.

    Args:
        text (str): Text (from a tweet) to be tokenized.

    Returns:
        :obj: `list` of `str`: List of tokens with twitter entities replaced by meta-
            words.

    Notes:
        See the docstring of `tokens_with_entities_replaced()` for a full list of the
            twitter entities replaced by meta-words.
    """
    #if preserve_case:
    #    tokenizer = reckless_tokenizer
    #else:
    #    raise ValueError("careful_tokenizer not implemented")
        #tokenizer = careful_tokenizer

    return tokens_with_entities_replaced( tokenizer.tokenize(text) )


def selected_word_features(corpus, cutoff=0.005):
    """
    Given a corpus of documents, return a dictionary of tokens and their relative weights.

    Rather than selecting all possible tokens, this function calculates the frequency of
    every token and only returns those which occur in greater than a certain percentage of
    documents.

    Args:
        corpus (:obj: `list` of `list` of `str`): Nested list of `str` tokens, each
            representing a unigram or word. Each `list` of tokens represents a single
            tokenized document.
        cutuff (float, optional): Decimal fraction of documents that a token must appear
            in to be selected. Defaults to 0.005 or 0.5%.

    Returns:
        :obj: `dict` of {str: int}: Dictionary containing the selected unigrams, paired
            with their weights. Weights are calculated by the size of the corpus divided
            by the number of documents that contain the unigram. This relative weighting
            calculation is known as the Inverse Document Frequency (tf-idf).

    """
    # Calculate word frequency and corpus size
    word_count = {}
    number_documents = 0
    for document in corpus:
        for word in set(document): # count number of documents, not total appearances
            try:
                word_count[word] += 1
            except KeyError:
                word_count[word] = 1
        number_documents += 1

    # Calculate threshold count from corpus size
    threshold = cutoff * number_documents

    # Return all words which pass the calculated threshold
    return { word: log(number_documents/count)
             for word,count in word_count.iteritems()
             if count > threshold }


def extracted_word_features(tokens, selected_words):
    """
    Generates a weighted feature for each selected_word for a given list of tokens.
    """
    return { word: (weight if word in tokens else 0)
             for word, weight in selected_words.iteritems() }


def partition_corpora(corpora, removable=None):
    "Partitions a number of corpora into halves for training and testing"
    training, testing = [], []
    for corpus in corpora:
        index_split = len(corpus) // 2
        training.append(corpus[:index_split])
        testing.append(corpus[index_split:])
    return training, testing


def ordered_list_of_dict(dictionary):
	"""
	Given a dictionary, return a list of its values, sorted by keys.

	Notes:
		Used to convert a complete feature set (generated by feature_set_generator) of a
			text into an array-like structure so it can be passed to the classifier.

	"""
	return [ v for k,v in sorted(dictionary.iteritems(), key=lambda t:t[0]) ]
